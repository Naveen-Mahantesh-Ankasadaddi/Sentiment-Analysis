


In [5]:
from fer import FER
import cv2

img = cv2.imread("happy.jpg")
detector = FER()
res = detector.detect_emotions(img)
[]
In [8]:
print(res)
[OrderedDict([('box', (177, 34, 86, 86)), ('emotions', {'angry': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happy': 1.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 0.0})])]
In [14]:
for i in res:
    for j in i:
        print()
OrderedDict([('box', (177, 34, 86, 86)), ('emotions', {'angry': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happy': 1.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 0.0})])
In [2]:
emotion, score = detector.top_emotion(img)
print(emotion,score)
happy 1.0
In [3]:
type(emotion),type(score)
Out[3]:
(str, numpy.float32)
In [20]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("happy.jpg")
detector = FER(mtcnn=False)
result = detector.detect_emotions(img)
print(result)
plt.imshow(img)
[OrderedDict([('box', (171, 28, 96, 96)), ('emotions', {'angry': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happy': 1.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 0.0})])]
Out[20]:
<matplotlib.image.AxesImage at 0x1ada6036ba8>

In [22]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("sad.jpg")
detector = FER(mtcnn=False)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('sad', 0.91)
Out[22]:
<matplotlib.image.AxesImage at 0x1ad8e06bd30>

In [23]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("angry.jpg")
detector = FER(mtcnn=False)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('angry', 0.96)
Out[23]:
<matplotlib.image.AxesImage at 0x1ad8f4d1ac8>

In [28]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("fearful.jpg")
detector = FER(mtcnn=False)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('fear', 0.92)
Out[28]:
<matplotlib.image.AxesImage at 0x1adb8e37518>

In [31]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("disg.jpg")
detector = FER(mtcnn=True)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('angry', 0.42)
Out[31]:
<matplotlib.image.AxesImage at 0x1adc1fde4a8>

In [26]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("neutral.jpg")
detector = FER(mtcnn=True)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('neutral', 0.88)
Out[26]:
<matplotlib.image.AxesImage at 0x1adb5239320>

In [27]:
from fer import FER
import matplotlib.pyplot as plt 

img = plt.imread("surprise.jpg")
detector = FER(mtcnn=True)
result = detector.top_emotion(img)
print(result)
plt.imshow(img)
('surprise', 0.95)
Out[27]:
<matplotlib.image.AxesImage at 0x1adb76cc358>

In [19]:
import cv2

from fer import FER

detector = FER(mtcnn=True) 

cap = cv2.VideoCapture(0)
while cap.isOpened():
    _, image = cap.read()

    result = detector.detect_emotions(image)
    
    print(result)
[]
[]
[]
[OrderedDict([('box', (232, 269, 196, 196)), ('emotions', {'angry': 0.05, 'disgust': 0.0, 'fear': 0.02, 'happy': 0.11, 'sad': 0.14, 'surprise': 0.06, 'neutral': 0.63})])]
[]
[]
[]
[]
[]
[]
[OrderedDict([('box', (226, 263, 209, 209)), ('emotions', {'angry': 0.08, 'disgust': 0.0, 'fear': 0.01, 'happy': 0.15, 'sad': 0.05, 'surprise': 0.07, 'neutral': 0.64})])]
[OrderedDict([('box', (223, 266, 212, 212)), ('emotions', {'angry': 0.06, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.09, 'sad': 0.18, 'surprise': 0.16, 'neutral': 0.47})])]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[OrderedDict([('box', (262, 281, 182, 182)), ('emotions', {'angry': 0.07, 'disgust': 0.0, 'fear': 0.04, 'happy': 0.05, 'sad': 0.1, 'surprise': 0.45, 'neutral': 0.29})])]
[OrderedDict([('box', (261, 275, 186, 186)), ('emotions', {'angry': 0.18, 'disgust': 0.03, 'fear': 0.06, 'happy': 0.06, 'sad': 0.19, 'surprise': 0.19, 'neutral': 0.27})])]
[]
[]
[OrderedDict([('box', (233, 263, 176, 176)), ('emotions', {'angry': 0.23, 'disgust': 0.0, 'fear': 0.05, 'happy': 0.15, 'sad': 0.22, 'surprise': 0.1, 'neutral': 0.24})])]
[OrderedDict([('box', (228, 256, 183, 183)), ('emotions', {'angry': 0.26, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.24, 'sad': 0.14, 'surprise': 0.04, 'neutral': 0.3})])]
[OrderedDict([('box', (232, 262, 178, 178)), ('emotions', {'angry': 0.2, 'disgust': 0.01, 'fear': 0.03, 'happy': 0.18, 'sad': 0.19, 'surprise': 0.13, 'neutral': 0.28})])]
[OrderedDict([('box', (239, 276, 170, 170)), ('emotions', {'angry': 0.19, 'disgust': 0.02, 'fear': 0.03, 'happy': 0.11, 'sad': 0.26, 'surprise': 0.14, 'neutral': 0.25})])]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[OrderedDict([('box', (254, 260, 189, 189)), ('emotions', {'angry': 0.07, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.05, 'sad': 0.15, 'surprise': 0.1, 'neutral': 0.6})])]
[OrderedDict([('box', (268, 241, 165, 165)), ('emotions', {'angry': 0.08, 'disgust': 0.0, 'fear': 0.05, 'happy': 0.04, 'sad': 0.61, 'surprise': 0.07, 'neutral': 0.14})])]
[OrderedDict([('box', (260, 242, 177, 177)), ('emotions', {'angry': 0.11, 'disgust': 0.0, 'fear': 0.05, 'happy': 0.03, 'sad': 0.66, 'surprise': 0.02, 'neutral': 0.12})])]
[OrderedDict([('box', (262, 239, 174, 174)), ('emotions', {'angry': 0.15, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.01, 'sad': 0.73, 'surprise': 0.02, 'neutral': 0.07})])]
[OrderedDict([('box', (247, 241, 193, 193)), ('emotions', {'angry': 0.11, 'disgust': 0.0, 'fear': 0.02, 'happy': 0.29, 'sad': 0.28, 'surprise': 0.04, 'neutral': 0.25})])]
[OrderedDict([('box', (251, 239, 192, 192)), ('emotions', {'angry': 0.02, 'disgust': 0.0, 'fear': 0.01, 'happy': 0.81, 'sad': 0.05, 'surprise': 0.06, 'neutral': 0.05})])]
[OrderedDict([('box', (259, 249, 188, 188)), ('emotions', {'angry': 0.07, 'disgust': 0.0, 'fear': 0.02, 'happy': 0.49, 'sad': 0.14, 'surprise': 0.07, 'neutral': 0.21})])]
[OrderedDict([('box', (261, 250, 188, 188)), ('emotions', {'angry': 0.06, 'disgust': 0.0, 'fear': 0.01, 'happy': 0.68, 'sad': 0.11, 'surprise': 0.03, 'neutral': 0.11})])]
[OrderedDict([('box', (264, 251, 184, 184)), ('emotions', {'angry': 0.06, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.28, 'sad': 0.24, 'surprise': 0.08, 'neutral': 0.32})])]
[OrderedDict([('box', (264, 251, 183, 183)), ('emotions', {'angry': 0.05, 'disgust': 0.0, 'fear': 0.02, 'happy': 0.35, 'sad': 0.19, 'surprise': 0.05, 'neutral': 0.35})])]
[OrderedDict([('box', (261, 247, 186, 186)), ('emotions', {'angry': 0.1, 'disgust': 0.0, 'fear': 0.02, 'happy': 0.18, 'sad': 0.17, 'surprise': 0.09, 'neutral': 0.43})])]
[OrderedDict([('box', (263, 248, 185, 185)), ('emotions', {'angry': 0.11, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.08, 'sad': 0.21, 'surprise': 0.06, 'neutral': 0.51})])]
[OrderedDict([('box', (267, 248, 186, 186)), ('emotions', {'angry': 0.08, 'disgust': 0.0, 'fear': 0.03, 'happy': 0.06, 'sad': 0.16, 'surprise': 0.07, 'neutral': 0.6})])]
[OrderedDict([('box', (269, 250, 187, 187)), ('emotions', {'angry': 0.04, 'disgust': 0.0, 'fear': 0.04, 'happy': 0.04, 'sad': 0.3, 'surprise': 0.02, 'neutral': 0.55})])]
[OrderedDict([('box', (270, 248, 191, 191)), ('emotions', {'angry': 0.08, 'disgust': 0.0, 'fear': 0.07, 'happy': 0.01, 'sad': 0.31, 'surprise': 0.03, 'neutral': 0.49})])]
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-19-b8645767acc5> in <module>
      9     _, image = cap.read()
     10 
---> 11     result = detector.detect_emotions(image)
     12 
     13     print(result)

~\Anaconda3\lib\site-packages\fer\fer.py in detect_emotions(self, img)
    234         emotion_labels = self._get_labels()
    235 
--> 236         face_rectangles = self.find_faces(img, bgr=True)
    237 
    238         gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

~\Anaconda3\lib\site-packages\fer\fer.py in find_faces(self, img, bgr)
    192             )
    193         elif self.__face_detector == "mtcnn":
--> 194             results = self._mtcnn.detect_faces(img)
    195             faces = [x["box"] for x in results]
    196 

~\Anaconda3\lib\site-packages\mtcnn\mtcnn.py in detect_faces(self, img)
    300         # We pipe here each of the stages
    301         for stage in stages:
--> 302             result = stage(img, result[0], result[1])
    303 
    304         [total_boxes, points] = result

~\Anaconda3\lib\site-packages\mtcnn\mtcnn.py in __stage1(self, image, scales, stage_status)
    340             img_y = np.transpose(img_x, (0, 2, 1, 3))
    341 
--> 342             out = self._pnet.predict(img_y)
    343 
    344             out0 = np.transpose(out[0], (0, 2, 1, 3))

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training_v1.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    990         max_queue_size=max_queue_size,
    991         workers=workers,
--> 992         use_multiprocessing=use_multiprocessing)
    993 
    994   def reset_metrics(self):

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training_arrays.py in predict(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)
    712         verbose=verbose,
    713         steps=steps,
--> 714         callbacks=callbacks)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    384 
    385         # Get outputs.
--> 386         batch_outs = f(ins_batch)
    387         if not isinstance(batch_outs, list):
    388           batch_outs = [batch_outs]

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs)
   3823 
   3824     fetched = self._callable_fn(*array_vals,
-> 3825                                 run_metadata=self.run_metadata)
   3826     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3827     output_structure = nest.pack_sequence_as(

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-> 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

KeyboardInterrupt: 
In [ ]:
